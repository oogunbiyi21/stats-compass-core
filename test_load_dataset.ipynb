{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bff0ec4",
   "metadata": {},
   "source": [
    "# Refactor and Test Dataset Loading Logic\n",
    "\n",
    "This notebook tests the refactored `load_dataset_tool.py` which now includes all dataset loading logic, removing the dependency on `datasets.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60bc3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets directory: /Users/tunjiogunbiyi/Dev/stats-compass-core/stats_compass_core/datasets\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, ConfigDict, Field\n",
    "from stats_compass_core.registry import registry\n",
    "from stats_compass_core.results import DataFrameLoadResult\n",
    "from stats_compass_core.state import DataFrameState\n",
    "\n",
    "# Define path to datasets directory (simulating the package structure)\n",
    "# We assume we are running this from the root of the repo\n",
    "_DATASETS_DIR = Path(\"stats_compass_core/datasets\")\n",
    "print(f\"Datasets directory: {_DATASETS_DIR.absolute()}\")\n",
    "print(f\"Exists: {_DATASETS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7724fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['TATASTEEL', 'Housing', 'Bukayo_Saka_7322']\n"
     ]
    }
   ],
   "source": [
    "def _list_available_datasets() -> list[str]:\n",
    "    \"\"\"List available sample datasets.\"\"\"\n",
    "    if not _DATASETS_DIR.exists():\n",
    "        return []\n",
    "    return [f.stem for f in _DATASETS_DIR.glob(\"*.csv\")]\n",
    "\n",
    "print(f\"Available datasets: {_list_available_datasets()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be88350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema defined successfully.\n"
     ]
    }
   ],
   "source": [
    "class LoadDatasetInput(BaseModel):\n",
    "    \"\"\"Input schema for load_dataset tool.\"\"\"\n",
    "    model_config = ConfigDict(populate_by_name=True)\n",
    "\n",
    "    name: str = Field(\n",
    "        description=f\"Name of the dataset to load. Available: {', '.join(_list_available_datasets())}\",\n",
    "    )\n",
    "    set_active: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Whether to set this as the active DataFrame\"\n",
    "    )\n",
    "\n",
    "print(\"Input schema defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37720f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dataset function defined.\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(state: DataFrameState, params: LoadDatasetInput) -> DataFrameLoadResult:\n",
    "    \"\"\"\n",
    "    Load a built-in sample dataset into the session state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = _DATASETS_DIR / f\"{params.name}.csv\"\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add to state\n",
    "        state.set_dataframe(df, name=params.name, operation=\"load_dataset\", set_active=params.set_active)\n",
    "        \n",
    "        return DataFrameLoadResult(\n",
    "            name=params.name,\n",
    "            rows=len(df),\n",
    "            columns=list(df.columns),\n",
    "            dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "            head=df.head().to_dict(orient=\"records\"),\n",
    "            is_active=params.set_active,\n",
    "        )\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        available = \", \".join(_list_available_datasets())\n",
    "        raise ValueError(f\"Dataset '{params.name}' not found. Available datasets: {available}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load dataset '{params.name}': {str(e)}\")\n",
    "\n",
    "print(\"load_dataset function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4909940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to load dataset 'Housing': 5 validation errors for DataFrameLoadResult\n",
      "success\n",
      "  Field required [type=missing, input_value={'name': 'Housing', 'rows...d'}], 'is_active': True}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
      "dataframe_name\n",
      "  Field required [type=missing, input_value={'name': 'Housing', 'rows...d'}], 'is_active': True}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
      "source\n",
      "  Field required [type=missing, input_value={'name': 'Housing', 'rows...d'}], 'is_active': True}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
      "shape\n",
      "  Field required [type=missing, input_value={'name': 'Housing', 'rows...d'}], 'is_active': True}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n",
      "message\n",
      "  Field required [type=missing, input_value={'name': 'Housing', 'rows...d'}], 'is_active': True}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n"
     ]
    }
   ],
   "source": [
    "# Test loading a valid dataset\n",
    "state = DataFrameState()\n",
    "params = LoadDatasetInput(name=\"Housing\")\n",
    "\n",
    "try:\n",
    "    result = load_dataset(state, params)\n",
    "    print(f\"Successfully loaded dataset: {result.name}\")\n",
    "    print(f\"Rows: {result.rows}, Columns: {len(result.columns)}\")\n",
    "    print(f\"Head: {result.head[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65979be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught expected error: Dataset 'NonExistentDataset' not found. Available datasets: TATASTEEL, Housing, Bukayo_Saka_7322\n"
     ]
    }
   ],
   "source": [
    "# Test error handling for invalid dataset\n",
    "try:\n",
    "    invalid_params = LoadDatasetInput(name=\"NonExistentDataset\")\n",
    "    load_dataset(state, invalid_params)\n",
    "except ValueError as e:\n",
    "    print(f\"Caught expected error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Caught unexpected error: {type(e).__name__}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stats Compass Core (Poetry)",
   "language": "python",
   "name": "stats-compass-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
